---
title: "Practice Lecture 20 MATH 342W Queens College"
author: "Professor Adam Kapelner"
date: "April 19, 2021"
---


# Regression Trees

Let's fit a regression tree. We will use the development package `YARF` which I've been hacking on now for a few years. The package internals are written in Java which we just installed above. Since `YARF` is not on CRAN, we install the package from my github including its dependency (if necessary) and then load it 

```{r}
if (!pacman::p_isinstalled(YARF)){
  pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
  pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev")
}
pacman::p_load(YARF)
```

The data will be fitting with the regression tree is a sine curve plus noise:

```{r}
pacman::p_load(tidyverse, magrittr)
n = 500
x_max = 10
x = runif(n, 0, x_max)
y = sin(x) + rnorm(n, 0, 0.3)
ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd = 0.6) 
```

Now we fit a regression tree to this model. Nevermind the `bootstrap_indices` and `calculate_oob_error` arguments. This will be clear why they are not defaults later in the semester.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, 
            bootstrap_indices = list(1 : n), calculate_oob_error = FALSE)
```

How "big" is this tree model?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

What are the "main" splits?

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
```

What does $g(x)$ look like?

```{r}
Nres = 1000
x_predict = data.frame(x = seq(0, x_max, length.out = Nres))
g = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = g), col = "blue")
```

Obviously overfit - but not that bad... let's try lowering the complexity by stopping the tree construction at a higher node size.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = 50, 
            bootstrap_indices = list(1 : n), calculate_oob_error = FALSE)
g = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = g), col = "blue")
```

Less overfitting now but now it's clearly underfit! We can play with the nodesize. Or we can use the model selection algorithm to pick the model (the nodesize).

# Regression Trees with Real Data

Now let's look at a regression tree model predicting medv in the Boston Housing data. We first load the data and do a training-test split:

```{r}
pacman::p_load(MASS)
data(Boston)
test_prop = 0.1
train_indices = sample(1 : nrow(Boston), round((1 - test_prop) * nrow(Boston)))
Boston_train = Boston[train_indices, ]
y_train = Boston_train$medv
X_train = Boston_train
X_train$medv = NULL
n_train = nrow(X_train)
```

And fit a tree model. The default hyperparameter, the node size is $N_0 = 5$.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(YARF)
tree_mod = YARFCART(X_train, y_train, 
            bootstrap_indices = list(1 : n_train), calculate_oob_error = FALSE)
```

What does the in-sample fit look like?

```{r}
y_hat_train = predict(tree_mod, Boston_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

Recall the linear model:

```{r}
linear_mod = lm(medv ~ ., Boston_train)
summary(linear_mod)$sigma
summary(linear_mod)$r.squared
```

The tree seems to win in-sample. Why? 

Is this a "fair" comparison?

Before we address this, let's illustrate the tree. 

```{r}
illustrate_trees(tree_mod, max_depth = 3, open_file = TRUE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Let's make the comparison fair by seeing what happens oos.

```{r}
test_indices = setdiff(1 : nrow(Boston), train_indices)
Boston_test = Boston[test_indices, ]
y_test = Boston_test$medv
X_test = Boston_test
X_test$medv = NULL
```

For the tree:

```{r}
y_hat_test_tree = predict(tree_mod, Boston_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

For the linear model:

```{r}
y_hat_test_linear = predict(linear_mod, Boston_test)
e = y_test - y_hat_test_linear
sd(e)
1 - sd(e) / sd(y_test)
```

The take-home message here is that the tree beats the linear model in future predictive performance but the only way to be truly convinced of this is to do the split over and over to get a sense of the average over the massive variability (like the previous demo) or to do CV to reduce the error of the estimate. 

Why does the regression tree beat the linear model? Let's see what's going on in the tree.

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

About how many observations are in each leaf?

```{r}
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

That's a very flexible model.

Let's see overfitting in action. Let's set nodesize to be one.

```{r}
tree_mod = YARFCART(X_train, y_train, nodesize = 1, 
            bootstrap_indices = 1 : n_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why is it not exactly 1 on average? I think it's because...

```{r}
data.table::uniqueN(y_train)
length(y_train)
```

Regardless of this point, this model is essentially giving each observation it's own y-hat, it's own personal guess which will be its own personal y. Just like linear modeling when $n = p + 1$ and nearest neighbors when $K = 1$. Let's see how bad the overfitting is:

```{r}
y_hat_train = predict(tree_mod, Boston_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

This is the expected behavior in perfect fitting.

```{r}
y_hat_test_tree = predict(tree_mod, Boston_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

Amazing it doesn't get clobbered completely! And its results are on-par with the non-overfit linear model. Trees are truly incredible and I still don't understand everything about them.

```{r}
rm(list = ls())
```

# Classification Trees

Let's get the cancer biopsy data:

```{r}
pacman::p_load(tidyverse, magrittr)
data(biopsy, package = "MASS")
biopsy %<>% na.omit %>% select(-ID)
colnames(biopsy) = c(
  "clump_thickness",
  "cell_size_uniformity",
  "cell_shape_uniformity",
  "marginal_adhesion",
  "epithelial_cell_size",
  "bare_nuclei",
  "bland_chromatin",
  "normal_nucleoli",
  "mitoses",
  "class"
)
```

Let's do a training-test split to keep things honest:

```{r}
test_prop = 0.1
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
n_train = nrow(X_train)
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```

Let's fit a tree:

```{r}
tree_mod = YARFCART(X_train, y_train, 
            bootstrap_indices = 1 : n_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(biopsy_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why would the average observations per node be larger than the nodesize which is 1?

```{r}
illustrate_trees(tree_mod, max_depth = 5, open_file = TRUE)
```

How are we doing in-sample?

```{r}
y_hat_train = predict(tree_mod, biopsy_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, biopsy_test)
mean(y_test != y_hat_test)
```

Still pretty good!

Now let's take a look at the linear SVM model.

```{r}
pacman::p_load(e1071)
svm_model = svm(X_train, y_train, kernel = "linear")
```

A couple of points:

* Reached max iterations to minimize the hinge loss. Seems like there are computational issues here.
* Note that we are relying on the $\lambda$ hyperparameter value for the hinge loss. On the homework, you will answer the question we never answered: how should the value of the hyperparameter be chosen?

Regardless, how did it do in-sample?

```{r}
y_hat_train = predict(svm_model, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, X_test)
mean(y_test != y_hat_test)
```

Maybe the model truly was linearly separable? Meaning, you don't get any added benefit from the tree if there are no interactions or non-linearities. Let's try a harder dataset. First, get a bunch of datasets from the UCI repository:

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
?adult
```

Let's use samples of 2,000 to run experiments:

```{r}
test_size = 2000
train_indices = sample(1 : nrow(adult), test_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL
n_train = nrow(X_train)
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Make a tree:

```{r}
tree_mod = YARFCART(X_train, y_train, 
            bootstrap_indices = 1 : n_train, calculate_oob_error = FALSE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(adult_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
illustrate_trees(tree_mod, max_depth = 5, open_file = TRUE)
```

In-sample?

```{r}
y_hat_train = predict(tree_mod, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

The warning was legit this time. What's it saying?

Looks like we overfit quite a bit! That's what nodesize of 1 does! Why is it the default? People found that even though it overfits, you still get good performance (as we've seen even with regression). I doubt people still use CART in production these days since this issue was fixed with bagging (we will get to this soon).

Let's see how the linear SVM does. Warning: this takes a while to compute:

```{r}
svm_model = svm(model.matrix(~ ., X_train), y_train, kernel = "linear")
```

In-sample?

```{r}
y_hat_train = predict(svm_model, model.matrix(~ ., X_train))
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, model.matrix(~ ., X_test))
mean(y_test != y_hat_test)
```

It seems (at least when I ran it at home), the linear SVM does much worse. Likely there are a lot of interactions in this dataset that the linear SVM must ignore because it's $\mathcal{H}$ candidate set is so limited!

Note: SVM train error is approximtely = SVM test error? Why? 

That's a usual scenario during underfitting in high n situations. There is no estimation error - only misspecification error and error due to ignorance. And those are the same among the training and test set.

