---
title: "Practice Lecture 12 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "March 10, 2021"
---

# Correlation zero means orthogonality

Let's generate some fake data. In this example we'll have one predictor which will be orthogonal to the centered response. We enforce the response to be centered by adding a column of 1's:

```{r}
n = 100; p = 2
Q = qr.Q(qr(cbind(1, matrix(rnorm(n * p), nrow = n))))
y = Q[, p + 1]
x = Q[, 2]
```

Let's make sure it's orthogonal:

```{r}
x %*% y
```

If they're orthogonal and y is mean-centered, what is the correlation?

```{r}
cor(x, y)
```

If the correlation is 0, what is $b_1$, the slope? It has to be zero. Thus $b_0$ has to be $bar{x}$. Since x was also orthogonalized to the vector of 1's, it's centered and hence has average = 0. So both intercept and slope are 0:

What is $b$?

```{r}
mod = lm(y ~ x)
coef(mod)
```

What is $R^2$? Since $x$ and $y$ are orthogonal... a projection onto the colspace of $X$ gets annhilated.

```{r}
summary(mod)$r.squared
```

# Random correlations are non-zero

```{r}
set.seed(1984)
n = 100
x = rnorm(n)
x = x - mean(x)
y = rnorm(n)
y = y - mean(y)
x
y
```

In this setup, $x$ and $y$ are centered Gaussian random vectors. Are they orthogonal?

```{r}
x %*% y
theta_in_rad = acos(x %*% y / sqrt(sum(x^2) * sum(y^2)))
theta_in_rad
theta_in_rad * 180 / pi
abs(90 - theta_in_rad * 180 / pi)
```

Nope... what about correlated?

```{r}
cor(x, y)
cor(x, y)^2
```

They *nearly* uncorrelated but they still have some correlation. How is this possible? 

There is "random chance"" AKA "chance capitalization"!

What about the best fitting line?

```{r}
mod = lm(y ~ x)
coef(mod)
```

Slope is about 0.09 which is small but non-zero.

What is $R^2$? Since $x$ and $y$ are nearly orthogonal... a projection onto the colspace of $X$ gets nearly annhilated.

```{r}
summary(mod)$r.squared
```

but not entirely. Lesson learned: random noise can be correlated with the response $y$ and give you the illusion of fit!


# The monotonicity of SSR (or $R^2$) with more features

As p increases, $R^2$ goes up. Here's a nice exercise:

```{r}
n = 200
y = rnorm(n)
Rsqs = array(NA, n)

#we know that Rsq = 0 for the null model (i.e. just regressing on the intercept)
Rsqs[1] = 0

#create a matrix with the correct number of rows but no columns
X = matrix(NA, nrow = n, ncol = 0)
X = cbind(1, X)

#for every new p, tack on a new random continuos predictor:
for (p_plus_one in 2 : n){
  X = cbind(X, rnorm(n))
  Rsqs[p_plus_one] = summary(lm(y ~ X))$r.squared
}
all(diff(Rsqs) > 0) #additional Rsq per new feature is positive
mean(diff(Rsqs)) #added Rsq per new feature
mean(diff(Rsqs)) * (n - 1) #check to ensure it's 100%
```

Now let's plot it and see what happens:

```{r}
pacman::p_load(ggplot2)
base = ggplot(data.frame(p_plus_one = 1 : n, Rsq = Rsqs))
base + geom_line(aes(x = p_plus_one, y = Rsq))
```

With each additional predictor, what happens to $R^2$?

```{r}
pacman::p_load(latex2exp)
base + 
  geom_line(aes(x = p_plus_one, y = c(0, diff(Rsq)))) + xlab("p + 1") + ylab(TeX("$\\Delta R^2$")) +
  geom_hline(yintercept = mean(diff(Rsqs)), col = "blue")
```

How can this possibly be?? The $x$'s are not related to $y$ whatsoever!!

Chance capitalization prevails. Each additional predictor picks up another dimension to add to the column space of $X$. Eventually, the projection explains *all* the variance. If $n = p + 1$, that matrix is square and of full rank, hence $\hat{y} = y$ and all residuals $e = 0$ since it is merely solving $n$ linearly independent equations.

So here's an idea. To get a perfect fit, just augment your design matrix with $n - (p + 1)$ random vectors and you get $R^2 = 100\%$!! There must be something wrong with this!!

Even if $p$ is large and $<n$, you are getting a lot of fit for free. So there's something wrong with this too!

This is called overfitting.

## Overfitting

Let's see how overfitting increases generalized estimation error.

```{r}
set.seed(1)
bbeta = c(1, 2, 3, 4)

#build training data
n = 100
X = cbind(1, rnorm(n), rnorm(n), rnorm(n))
y = X %*% bbeta + rnorm(n, 0, 0.3)
#\amthbb{D}

#build test data
n_star = 1000
X_star = cbind(1, rnorm(n_star), rnorm(n_star), rnorm(n_star))
y_star = X_star %*% bbeta + rnorm(n_star, 0, 0.3)

all_betas = matrix(NA, n, n)
all_betas[4, 1 : 4] = coef(lm(y ~ 0 + X))
in_sample_rmse_by_p = array(NA, n)
for (j in 5 : n){
  X = cbind(X, rnorm(n))
  lm_mod = lm(y ~ 0 + X)
  all_betas[j, 1 : j] = coef(lm_mod)
  y_hat = X %*% all_betas[j, 1 : j]
  in_sample_rmse_by_p[j] = sqrt(mean((y - y_hat)^2))
}
ggplot(data.frame(num_features = 1 : n, in_sample_rmse_by_p = in_sample_rmse_by_p)) + 
  geom_point(aes(x = num_features, y = in_sample_rmse_by_p))


all_betas[4 : n, 1 : 10]
b_error_by_p = rowSums((all_betas[, 1 : 4] - matrix(rep(bbeta, n), nrow = n, byrow = TRUE))^2)
ggplot(data.frame(num_features = 1 : n, b_error_by_p = b_error_by_p)) + 
  geom_point(aes(x = num_features, y = b_error_by_p))

#look at out of sample error with random features too
oos_rmse_by_p = array(NA, n)
for (j in 5 : n){
  X_star = cbind(X_star, rnorm(n))
  y_hat_star = X_star %*% all_betas[j, 1 : j]
  oos_rmse_by_p[j] = sqrt(mean((y_star - y_hat_star)^2))
}
ggplot(data.frame(num_features = 1 : n, oos_rmse_by_p = oos_rmse_by_p)) + 
  geom_point(aes(x = num_features, y = oos_rmse_by_p))
```

Let's look at overfitting from another angle. Let's generate a linear model ($f = h^* \in \mathcal{H}$) and let $\epsilon$ be random noise (the error due to ignorance) for $\mathbb{D}$ featuring $n = 2$.

This simulation is random, but to ensure it looks the same to me right now as it does in class (and to you at home), let's "set the seed" so it is deterministic.

```{r}
set.seed(1003)
```

Now let's "randomly generate" the data:

```{r}
n = 2
beta_0 = 1
beta_1 = 1
x = rnorm(n)
#best possible model
h_star_x = beta_0 + beta_1 * x

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon

#scatterplot it
pacman::p_load(ggplot2)
basic = ggplot(data.frame(x = x, y = y, h_star_x = h_star_x), aes(x, y)) +
  geom_point() +
  xlim(-4, 4) + ylim(-5, 5)
basic
```

Let's now fit a linear model to this and plot:

```{r}
mod = lm(y ~ x)
b_0 = coef(mod)[1]
b_1 = coef(mod)[2]
basic + geom_abline(intercept = b_0, slope = b_1, col = "blue")
```

Note that obviously:

```{r}
summary(mod)$r.squared
summary(mod)$sigma #RMSE - technically cannot divide by zero so cannot estimate the RMSE so we use s_e instead
sd(mod$residuals) #s_e
```

And let's plot the true function $h^*$ below as well in green:

```{r}
basic_and_lines = basic + 
  geom_abline(intercept = b_0, slope = b_1, col = "blue") +
  geom_abline(intercept = beta_0, slope = beta_1, col = "green") + 
  geom_segment(aes(x = x, y = h_star_x, xend = x, yend = y), col = "red")
basic_and_lines
```

The red lines are the epsilons:

```{r}
epsilon
```

Now let's envision some new data not in $\mathbb{D}$. We will call this "out of sample" (oos) since $\mathbb{D}$ defined our "sample" we used to build the model. For the oos data, we predict on it using our linear model $g$ which is far from the best linear model $h^*$ and we look at its residuals $e$:

```{r}
n_new = 50
x_new = rnorm(n_new)
h_star_x_new = beta_0 + beta_1 * x_new
epsilon_new = rnorm(n_new)
y_new = h_star_x_new + epsilon_new
y_hat_new = b_0 + b_1 * x_new

df_new = data.frame(x = x_new, y = y_new, h_star_x = h_star_x_new, y_hat = y_hat_new, e = y - y_hat_new)

basic_and_lines + 
  geom_point(data = df_new) + 
  geom_segment(data = df_new, aes(x = x, y = y_hat_new, xend = x, yend = y), col = "purple")
```

Instead of the residuals let's look at its true errors, epsilon, the distance from $h^*$:

```{r}
basic_and_lines + 
  geom_point(data = df_new) + 
  geom_segment(data = df_new, aes(x = x, y = h_star_x, xend = x, yend = y), col = "darkgrey")
```

The errors that the overfit model are worse than the errors made by the best model. In other words, the residual standard error on the new "out of sample" data is much larger than the actual epsilon standard error:

```{r}
sd(df_new$e)
sd(epsilon_new)
```

How did we get in this mess? We can see from the picture we are using the grey line as $g$ but we should be using the green line $h^*$. We are using the grey line because we used the original $\epsilon$ to fit. BAD idea - won't generalize to the future.

```{r}
rm(list = ls())
```

The problem is bad for $n = 2$. But is it always bad? Let's take a look at $n = 100$ with a strong linear relationship with one predictor. 

```{r}
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
x = runif(n, xmin, xmax)
#best possible model
h_star_x = beta_0 + beta_1 * x

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

The true relationship is plotted below in green.

```{r}
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y)) +
  geom_point() +
  geom_abline(intercept = beta_0, slope = beta_1, col = "darkgreen")
basic
```

And the estimated line $g$ (in blue) is pretty close:

```{r}
mod = lm(y ~ x)
b = coef(mod)
basic +
  geom_abline(intercept = b[1], slope = b[2], col = "blue")
```

They're basically right on top of each other: estimation error near zero.

```{r}
b
c(beta_0, beta_1)
```

And the $R^2$ is:

```{r}
summary(mod)$r.squared
```

Not great... plenty of room for overfitting the nonsense epsilons.

Now what happens if we add a random predictor? 

* We know that $R^2$ will go up. 
* But since this predictor is random, it is independent of the best model $h^*(x)$, hence it will constitute *overfitting*. 
* Overfitting is bad because it induces estimation error and $g$ will diverge from $h*$ (previous demo)
* This divergence leads to bad oos error (generalization error) meaning subpar predictions when we actually use the model in the future

Okay - but how bad? It only depends on how much $g$ diverges from $h^*$. Let's look at this divergence slowly. We create a new oos data set made from evenly spaced $x$ values across its range and random values of the nonsense predictors. We use this to calculated oos $s_e$.

```{r}
p_fake = 98
set.seed(1984)
X = data.frame(matrix(c(x, rnorm(n * p_fake)), ncol = 1 + p_fake, nrow = n))
mod = lm(y ~ ., X)
t(t(coef(mod)))
y_hat = predict(mod, X)
s_e_in_sample = sd(y - y_hat)

nstar = 1000
set.seed(1984)
Xstar = matrix(c(seq(xmin, xmax, length.out = nstar), rnorm(nstar * p_fake)), ncol = 1 + p_fake)
y_stars = beta_0 + beta_1 * Xstar[, 1] + rnorm(nstar)
y_hat_stars = cbind(1, Xstar) %*% as.matrix(coef(mod))
s_e_oos = sd(y_stars - y_hat_stars)
basic_with_yhat = basic +
  geom_point(data = data.frame(x = x, y = y_hat), aes(x = x, y = y), col = "purple") + 
  xlim(0, 1) + ylim(-3, 8) +
  ggtitle(
    paste("Linear Model with", p_fake, "fake predictors"), 
    paste("Rsq =", round(summary(mod)$r.squared * 100, 2), "percent, in-sample s_e =", round(s_e_in_sample, 2), "and oos s_e =", round(s_e_oos, 2)))
basic_with_yhat

basic_with_yhat +
  geom_line(data = data.frame(x = Xstar[, 1], y = y_hat_stars), aes(x = x, y = y), col = "orange")
```

Lesson: it takes a bit of time to overfit badly. Don't worry about a few extra degrees of freedom if you have $n$ much larger than $p$. But it will eventually be corrosive!


# Assessing overfitting in practice

Let's examine this again. This time we use one data set which is split between training and testing.

```{r}
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
p = 50
X = matrix(runif(n * p, xmin, xmax), ncol = p)

#best possible model - only one predictor matters!
h_star_x = beta_0 + beta_1 * X[,1 ]

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

Now we split $\mathbb{D}$ into training and testing. We define $K$ first, the inverse proportion of the test size.

```{r}
K = 5 #i.e. the test set is 1/5th of the entire historical dataset

#a simple algorithm to do this is to sample indices directly
test_indices = sample(1 : n, 1 / K * n)
train_indices = setdiff(1 : n, test_indices)

#now pull out the matrices and vectors based on the indices
X_train = X[train_indices, ]
y_train = y[train_indices]
X_test = X[test_indices, ]
y_test = y[test_indices]

#let's ensure these are all correct
dim(X_train)
dim(X_test)
length(y_train)
length(y_test)
```

Now let's fit the model $g$ to the training data and compute in-sample error metrics:

```{r}
mod = lm(y_train ~ ., data.frame(X_train))
summary(mod)$r.squared
sd(mod$residuals)
```

Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:

```{r}
y_hat_oos = predict(mod, data.frame(X_test))
oos_residuals = y_test - y_hat_oos
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2)
sd(oos_residuals)
```



MUCH worse!! Why? We overfit big time...

Can we go back now and fit a new model and see how we did? NO...

So how are we supposed to fix a "bad" model? We can't unless we do something smarter. We'll get there.



correct the insample RMSE demo above...



# Nonlinear Linear Regression: Polynomial Regression

Let's generate a polynomial model of degree 2 ($f = h^* \in \mathcal{H}$) and let $\epsilon$ be random noise (the error due to ignorance) for $\mathbb{D}$ featuring $n = 2$.

```{r}
set.seed(1003)
n = 25
beta_0 = 1
beta_1 = 0
beta_2 = 1
x = runif(n, -2, 5)
#best possible model
h_star_x = beta_0 + beta_1 * x + beta_2 * x^2

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon

#scatterplot it
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y)) +
  geom_point()
basic
```

Let's try to estimate with a line:

```{r}
linear_mod = lm(y ~ x)
b_linear = summary(linear_mod)$coef
basic + geom_abline(intercept = b_linear[1], slope = b_linear[2], col = "red")
```

The relationship is "underfit". $\mathcal{H}$ is not rich enough right now to express something close to $f(x)$. But it is better than the null model!

Now let's do a polynomial regression of degree two. Let's do so manually:

```{r}
X = as.matrix(cbind(1, x, x^2))
b = solve(t(X) %*% X) %*% t(X) %*% y
b
c(beta_0, beta_1, beta_2)
```

These are about the same as the $\beta_0, \beta_1$ and $\beta_2$ as defined in $f(x)$ the true model. In order to graph this, we can no longer use the routine `geom_abline`, we need to use `stat_function`.

```{r}
plot_function_degree_2 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2
}

basic + stat_function(fun = plot_function_degree_2, args = list(b = b), col= "darkgreen")
```

Now let's try polynomial of degree 3:

```{r}
X = as.matrix(cbind(1, x, x^2, x^3))
b = solve(t(X) %*% X) %*% t(X) %*% y
b

plot_function_degree_3 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3
}

basic + stat_function(fun = plot_function_degree_3, args = list(b = b), col= "darkgreen")
```
Still the same. Why? The $x^3$ term is like adding one "nonsense" predictor. One nonsense predictor marginally affects $R^2$ but it doesn't matter too much.

Now let's try polynomial of degree 8:

```{r}
X = as.matrix(cbind(1, x, x^2, x^3, x^4, x^5, x^6, x^7, x^8))
b = solve(t(X) %*% X) %*% t(X) %*% y
b

plot_function_degree_8 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 
}

basic + stat_function(fun = plot_function_degree_8, args = list(b = b), col= "darkgreen")
```

We are seeing now a little bit of "overfitting" in the edge(s). We now have $p=11$ and $n=100$. We can do a lot worse!

Let's learn how to do this in R first without having to resort to manual linear algebra. R has a function called "poly" that can be used *inside* formula declarations.

Let's first fit the degree 2 model:

```{r}
degree_2_poly_mod = lm(y ~ poly(x, 2, raw = TRUE))
head(model.matrix(~ poly(x, 2, raw = TRUE))) #the model matrix for this regression - just to check
b_poly_2 = coef(degree_2_poly_mod)
b_poly_2
summary(degree_2_poly_mod)$r.squared
```

Let's go on a slight tangent. And look at this regression without using the raw polynomial.

```{r}
Xmm = model.matrix(~ poly(x, 10))
head(Xmm) #the model matrix for this regression - just to check
Xmm[, 1] %*% Xmm[, 2]
Xmm[, 2] %*% Xmm[, 2]
Xmm[, 2] %*% Xmm[, 3]
Xmm[, 3] %*% Xmm[, 3]
```

Are thse orthogonal polynomials? How is the `poly` function without `raw = TRUE` working to generate a model matrix?

```{r}
degree_2_orthog_poly_mod = lm(y ~ poly(x, 2))
b_poly_2 = coef(degree_2_orthog_poly_mod)
b_poly_2
summary(degree_2_orthog_poly_mod)$r.squared
```

Why is this a good idea to use orthogonal polynomials?

Same as we got before! We use "raw" polynomials to keep them interpretable and on the same scale as the manual models we were fitting.

Now let's do polynomial of degree 13:

```{r}
degree_13_poly_mod = lm(y ~ poly(x, 13, raw = TRUE))
b_poly_13 = coef(degree_13_poly_mod)

plot_function_degree_13 = function(x, b){
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x^3 + b[5] * x^4 + b[6] * x^5 + b[7] * x^6 + b[8] * x^7 + b[9] * x^8 + b[10] * x^9  + b[11] * x^10 + b[12] * x^11 + b[13] * x^12 + b[14] * x^13
}

basic + stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple")# + ylim(c(0, 25)) #+ xlim(c(-2, 5.2))
```

What's happening for small values of $x$ (and a bit for large values)? This is called [Runge's Phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) meaning that the boundary activity of high-order polynomials has very large derivatives. Let's go back to the same scale as before and see what's happening:

```{r}
basic + 
  coord_cartesian(xlim = c(-2, 5), ylim = c(-3, 25)) + 
  stat_function(fun = plot_function_degree_13, args = list(b = b_poly_13), col = "purple")
```

## Eigendecomposition in R

```{r}
B = array(seq(1, 4), dim = c(2, 2))
B
eigen_decomp = eigen(B)
V = eigen_decomp$vectors
V
v1 = V[, 1, drop = FALSE]
v2 = V[, 2, drop = FALSE]
lambdas = eigen_decomp$values
lambdas
lambda1 = lambdas[1]
lambda2 = lambdas[2]
B %*% v1
lambda1 * v1
B %*% v2 
lambda2 * v2
B %*% v2 == lambda2 * v2 #why not?
#diagonolization
V %*% diag(lambdas) %*% solve(V)
```